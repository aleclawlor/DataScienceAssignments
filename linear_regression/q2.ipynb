{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statewide-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes\n",
    "from random import shuffle\n",
    "from numpy import rint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-farming",
   "metadata": {},
   "source": [
    "**Part A: Fit a Linear Regression model to the diabetes dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "danish-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data = load_diabetes()\n",
    "diabetes_features = diabetes_data.data\n",
    "diabetes_targets = diabetes_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latest-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_obj = LinearRegression().fit(diabetes_features, diabetes_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "thick-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regression_obj.predict(diabetes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reduced-castle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5177494254132934"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared = regression_obj.score(diabetes_features, diabetes_targets)\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naked-geology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_exact_predictions = sum(diabetes_targets[predictions == diabetes_targets])\n",
    "proportion_exact = num_exact_predictions / len(predictions)\n",
    "proportion_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-interview",
   "metadata": {},
   "source": [
    "0 of our predictions were exactly correct, whereas the r^2 value was .51. This implies that while the regression line does provide exact predictions, the model itself is middle-of-the-pack in terms of fitting the sample data (which we hope also reflects performance on the population). This is expected, because we should not expect our regression line to perfectly fit many of the points exactly, if any. The point of the regression model is to best fit the linear relationship between the data, so to expect one line to go through many of the data points while also modelling the rest of the data is unrealistic. Thus, we cannot say that the model is ineffective because it does not exactly predict any of the target values perfectly. It may get very close to exact points, but by the nature of how the model is fit, we cannot expect it to perfectly predict every point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-publication",
   "metadata": {},
   "source": [
    "**Part B: Randomly split row indices of instances of the data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stupid-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_data_samples = len(diabetes_features)\n",
    "num_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "swedish-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of indices and randomly shuffle them to generate test/training sets\n",
    "data_indices = list(range(0, num_data_samples))\n",
    "shuffle(data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "happy-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = data_indices[0:300]\n",
    "testing_indices = data_indices[300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stunning-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = diabetes_features[training_indices]\n",
    "testing_set = diabetes_features[testing_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suburban-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression().fit(training_set, diabetes_targets[training_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "saved-phone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5258839489557672"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared_train = regression.score(training_set, diabetes_targets[training_indices])\n",
    "r_squared_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "touched-malawi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47848698142628354"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared_test = regression.score(testing_set, diabetes_targets[testing_indices])\n",
    "r_squared_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-words",
   "metadata": {},
   "source": [
    "The r^2 value for the training set is .52, as opposed to the smaller r^2 value of .477 for the testing set. The values are not the same, but the small difference suggests that the model forms slightly worse on the population data (testing set) than on the training data. The training set, then, represents a better fit, which should not come as surprising, since the model is fit based off of the training set. Since we fit the model with data from the training set, we can expect that the goodness of fit will be at least a little higher for the training set. Because we randomly selected indices, however, we are not seeing a massive difference in goodness of fit between training and testing set, since the population data is more or less accurately reflected in the sample data due to the random nature of how we selected indices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DataScienceEnv)",
   "language": "python",
   "name": "datascienceenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
